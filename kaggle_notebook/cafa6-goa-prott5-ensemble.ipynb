{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAFA6 GOA + ProtT5 Ensemble\n",
    "\n",
    "Simple ensemble of GOA and ProtT5 predictions with GO hierarchy propagation.\n",
    "\n",
    "Required dataset: `ymuroya47/cafa6-goa-predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import heapq\n",
    "\n",
    "# Paths\n",
    "COMPETITION_DATA = '/kaggle/input/cafa-6-protein-function-prediction'\n",
    "GOA_DATA = '/kaggle/input/cafa6-goa-predictions'\n",
    "GOA_PATH = f'{GOA_DATA}/goa_submission.tsv'\n",
    "PROTT5_PATH = f'{GOA_DATA}/prott5_interpro_predictions.tsv'\n",
    "GO_OBO_PATH = f'{COMPETITION_DATA}/Train/go-basic.obo'\n",
    "OUTPUT_PATH = 'submission.tsv'\n",
    "\n",
    "# Weights\n",
    "WEIGHT_GOA = 0.55\n",
    "WEIGHT_PROTT5 = 0.45\n",
    "\n",
    "# GO Roots\n",
    "GO_ROOTS = {\"GO:0003674\", \"GO:0008150\", \"GO:0005575\"}\n",
    "\n",
    "# Parameters (high TOP_K for better score)\n",
    "TOP_K = 250\n",
    "MIN_SCORE = 0.001\n",
    "POWER_SCALE = 0.8\n",
    "MAX_SCORE = 0.93\n",
    "\n",
    "print('Configuration loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_ids():\n",
    "    test_fasta = f'{COMPETITION_DATA}/Test/testsuperset.fasta'\n",
    "    ids = set()\n",
    "    with open(test_fasta, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                header = line[1:].strip().split()[0]\n",
    "                if '|' in header:\n",
    "                    header = header.split('|')[1]\n",
    "                ids.add(header)\n",
    "    print(f'Loaded {len(ids):,} test proteins')\n",
    "    return ids\n",
    "\n",
    "test_ids = load_test_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_go_ontology():\n",
    "    print('Parsing GO ontology...')\n",
    "    term_parents = defaultdict(set)\n",
    "\n",
    "    with open(GO_OBO_PATH, 'r') as f:\n",
    "        current_id = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('id: '):\n",
    "                current_id = line.split('id: ')[1].strip()\n",
    "            elif line.startswith('is_a: ') and current_id:\n",
    "                parent = line.split()[1].strip()\n",
    "                term_parents[current_id].add(parent)\n",
    "            elif line.startswith('relationship: part_of ') and current_id:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    parent = parts[2].strip()\n",
    "                    term_parents[current_id].add(parent)\n",
    "\n",
    "    ancestors_map = {}\n",
    "\n",
    "    def get_ancestors(term):\n",
    "        if term in ancestors_map:\n",
    "            return ancestors_map[term]\n",
    "        parents = term_parents.get(term, set())\n",
    "        all_anc = set(parents)\n",
    "        for p in parents:\n",
    "            all_anc |= get_ancestors(p)\n",
    "        ancestors_map[term] = all_anc\n",
    "        return all_anc\n",
    "\n",
    "    for term in tqdm(list(term_parents.keys()), desc='Building ancestors'):\n",
    "        get_ancestors(term)\n",
    "\n",
    "    print(f'Cached {len(ancestors_map):,} GO terms')\n",
    "    return ancestors_map\n",
    "\n",
    "ancestors_map = parse_go_ontology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(filepath, allowed_proteins, desc='Loading'):\n",
    "    preds = defaultdict(dict)\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in tqdm(f, desc=desc):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            protein = parts[0].strip()\n",
    "            if '|' in protein:\n",
    "                protein = protein.split('|')[1]\n",
    "            if protein not in allowed_proteins:\n",
    "                continue\n",
    "            go_term = parts[1].strip()\n",
    "            try:\n",
    "                score = float(parts[2])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if go_term in preds[protein]:\n",
    "                preds[protein][go_term] = max(preds[protein][go_term], score)\n",
    "            else:\n",
    "                preds[protein][go_term] = score\n",
    "    return dict(preds)\n",
    "\n",
    "print('Loading GOA...')\n",
    "goa_preds = load_predictions(GOA_PATH, test_ids, 'GOA')\n",
    "print(f'GOA proteins: {len(goa_preds):,}')\n",
    "\n",
    "print('Loading ProtT5...')\n",
    "prott5_preds = load_predictions(PROTT5_PATH, test_ids, 'ProtT5')\n",
    "print(f'ProtT5 proteins: {len(prott5_preds):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_predictions(goa_preds, prott5_preds):\n",
    "    print('Merging predictions...')\n",
    "    all_proteins = set(goa_preds.keys()) | set(prott5_preds.keys())\n",
    "    merged = {}\n",
    "    for pid in tqdm(all_proteins, desc='Merging'):\n",
    "        a = goa_preds.get(pid, {})\n",
    "        b = prott5_preds.get(pid, {})\n",
    "        if not a and not b:\n",
    "            continue\n",
    "        terms = set(a.keys()) | set(b.keys())\n",
    "        merged[pid] = {}\n",
    "        for t in terms:\n",
    "            s1 = a.get(t, 0.0)\n",
    "            s2 = b.get(t, 0.0)\n",
    "            if s1 > 0.0 and s2 > 0.0:\n",
    "                merged[pid][t] = WEIGHT_GOA * s1 + WEIGHT_PROTT5 * s2\n",
    "            else:\n",
    "                merged[pid][t] = s1 if s1 > 0.0 else s2\n",
    "    return merged\n",
    "\n",
    "merged = merge_predictions(goa_preds, prott5_preds)\n",
    "print(f'Merged proteins: {len(merged):,}')\n",
    "\n",
    "# Free memory\n",
    "del goa_preds, prott5_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_propagation(base_scores, ancestors_map):\n",
    "    upd = dict(base_scores)\n",
    "    for term, score in base_scores.items():\n",
    "        if term in GO_ROOTS:\n",
    "            continue\n",
    "        for anc in ancestors_map.get(term, ()):\n",
    "            prev = upd.get(anc)\n",
    "            if prev is None or score > prev:\n",
    "                upd[anc] = score\n",
    "    for root in GO_ROOTS:\n",
    "        upd[root] = 1.0\n",
    "    return upd\n",
    "\n",
    "def power_scaling(scores, power=0.80, max_score=0.93):\n",
    "    out = dict(scores)\n",
    "    non_root = [s for t, s in out.items() if t not in GO_ROOTS]\n",
    "    if not non_root:\n",
    "        for r in GO_ROOTS:\n",
    "            out[r] = 1.0\n",
    "        return out\n",
    "    mx = max(non_root)\n",
    "    if mx <= 0.0 or mx >= max_score:\n",
    "        for r in GO_ROOTS:\n",
    "            out[r] = 1.0\n",
    "        return out\n",
    "    inv = 1.0 / mx\n",
    "    for t in list(out.keys()):\n",
    "        if t in GO_ROOTS:\n",
    "            continue\n",
    "        val = (out[t] * inv) ** power * max_score\n",
    "        out[t] = min(1.0, val)\n",
    "    for r in GO_ROOTS:\n",
    "        out[r] = 1.0\n",
    "    return out\n",
    "\n",
    "def topk_filter(scores, k):\n",
    "    return heapq.nlargest(k, scores.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing proteins...')\n",
    "output_lines = []\n",
    "\n",
    "for pid in tqdm(merged, desc='Processing'):\n",
    "    base = merged[pid]\n",
    "    if not base:\n",
    "        continue\n",
    "    # 1. Positive propagation\n",
    "    pos = positive_propagation(base, ancestors_map)\n",
    "    # 2. Power scaling\n",
    "    scaled = power_scaling(pos, POWER_SCALE, MAX_SCORE)\n",
    "    # 3. Top-K filtering\n",
    "    top_terms = topk_filter(scaled, TOP_K)\n",
    "    # 4. Write predictions\n",
    "    for go_term, score in top_terms:\n",
    "        if score >= MIN_SCORE:\n",
    "            output_lines.append(f'{pid}\\t{go_term}\\t{score:.4f}')\n",
    "\n",
    "print(f'Total predictions: {len(output_lines):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Saving to {OUTPUT_PATH}...')\n",
    "with open(OUTPUT_PATH, 'w') as f:\n",
    "    f.write('\\n'.join(output_lines))\n",
    "\n",
    "import os\n",
    "size_mb = os.path.getsize(OUTPUT_PATH) / 1024 / 1024\n",
    "print(f'\\nSubmission saved: {OUTPUT_PATH}')\n",
    "print(f'  Total predictions: {len(output_lines):,}')\n",
    "print(f'  File size: {size_mb:.1f} MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
